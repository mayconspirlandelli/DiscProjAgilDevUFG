# from crewai import Agent, Task, Crew, Process
# from crewai_tools import ScrapeWebsiteTool, WebsiteSearchTool
# from crewai import LLM
# import json
# from dotenv import load_dotenv
# import os
# from pydantic import BaseModel
# from crewai_tools import FirecrawlSearchTool

# # P√°gina de exemplo (troque pela URL que desejar)
# #url = "https://escoladepos.ufg.br/cursos/atendimento-de-criancas-e-adolescentes-vitimas-ou-testemunhas-de-violencia/"
# url = "https://escoladepos.ufg.br/cursos/banco-de-dados-com-big-data/"

# # Carregar vari√°veis de ambiente do arquivo .env
# load_dotenv()  # Isso carrega as vari√°veis do .env para os.environ

# GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")


# llm = LLM(
#     model="gemini/gemini-2.0-flash",
#     temperature=0,
#     api_key=GOOGLE_API_KEY,
# )

# # llmGemma = LLM(
# #     model="ollama/gemma3:270m",
# #     base_url="http://localhost:11434",
# #     temperature=0
# # )


# # Ferramenta para raspagem de site
# scraper_tool = ScrapeWebsiteTool()


# # Agente que usa a ferramenta de scraping
# agente_extracao = Agent(
#     role="Agente de Coleta de Informa√ß√µes de Sites",
#     goal="Extrair informacoes dos curso de p√≥s-gradua√ß√£o da UFG. Voce deve caputra informa√ßoes sobre o curso, como nome, descri√ß√£o, data de in√≠cio e valor.",
#     backstory="Especialista em web scraping de lojas online e coleta de informa√ß√µes relevantes.",
#     verbose=True,
#     memory=True,
#     llm=llm,
#     tools=[scraper_tool]
# )


# # Tarefa que instrui o agente a usar a ferramenta
# extrair_informacoes_site = Task(
#     description=f"""
#     Acesse o site {url} e extraia informa√ßoes relativas ao curso de p√≥s-gradua√ß√£o da UFG.
#     Para cada curso, colete:
#     - Nome do curso
#     - Descri√ß√£o resumida
#     - Carga hor√°rio do curso
#     - Curso √© on-line, presencial ou h√≠brido
#     - Informa√ß√µes sobre o curso
#     - Edital est√° dipon√≠vel ou n√£o.
#     - Data de in√≠cio do curso
#     - Qual valor da mensalidade do curso.

#     Formate a sa√≠da como uma lista em JSON.
#     """,
#     expected_output="Uma lista JSON contendo nome, descri√ß√£o e pre√ßo de cada produto.",
#     tools=[scraper_tool],
#     agent=agente_extracao
# )

# agente_formatador_json = Agent(
#     role="Agente Responsavel por Formata√ß√£o JSON",
#     goal="Extrair lista de produtos, descri√ß√µes e pre√ßos de um site de e-commerce",
#     backstory="Especialista em web scraping de lojas online e coleta de informa√ß√µes relevantes.",
#     verbose=True,
#     memory=True,
#     llm=llm,
#         tools=[scraper_tool]
# )

# formatar_json = Task(
#     description=f"""
#     Formate a sa√≠da como uma lista em JSON com os seguintes campos para cada produto coletado:
#     - Nome do curso,
#     - Descri√ß√£o resumida,
#     - Carga hor√°rio do curso,
#     - Curso √© on-line, presencial ou h√≠brido,
#     - Informa√ß√µes sobre o curso,
#     - Edital est√° dipon√≠vel ou n√£o,
#     - Data de in√≠cio do curso,
#     - Qual valor da mensalidade do curso.
#     """,
#     expected_output="Uma lista JSON contendo todos as informa√ß√µes coletadas do site.",
#     tools=[scraper_tool],
#     llm=llm,
#     agent=agente_formatador_json
# )

# # Criar a equipe e processar
# equipe = Crew(
#     agents=[agente_extracao, agente_formatador_json],
#     tasks=[extrair_informacoes_site, formatar_json],
#     process=Process.sequential,
#     #llm=llmGemma,
#     verbose=True
# )


# # Executar
# resultado = equipe.kickoff(inputs={'url': url})

# print(resultado)

# with open("resultado_scraping.json", "w", encoding="utf-8") as f:
#      json.dump(saida_json, f, ensure_ascii=False, indent=2)

# print("‚úÖ Resultado salvo em 'resultado_scraping.json'")




# ================================================

from crewai import Agent, Task, Crew, Process
from crewai_tools import ScrapeWebsiteTool
from crewai import LLM
import json
from dotenv import load_dotenv
import os
from pydantic import BaseModel
from typing import List

# P√°gina de exemplo (troque pela URL que desejar)
#url = "https://escoladepos.ufg.br/cursos/atendimento-de-criancas-e-adolescentes-vitimas-ou-testemunhas-de-violencia/"
url = "https://escoladepos.ufg.br/cursos/banco-de-dados-com-big-data/"

# Carregar vari√°veis de ambiente
load_dotenv()
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")


USE_LOCAL = os.getenv("USE_LOCAL_MODEL", "false").lower() == "true"

if USE_LOCAL:
    llm = LLM(
        #model="ollama/gemma3:270m",
        model="ollama/qwen3:1.7b",
        base_url="http://localhost:11434",
        temperature=0
    )
else:
    GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
    llm = LLM(
        model="gemini/gemini-2.0-flash",
        temperature=0,
        api_key=GOOGLE_API_KEY,
    )


# Modelo Pydantic para estruturar a sa√≠da
class CursoInfo(BaseModel):
    nome: str
    descricao: str
    carga_horaria: str
    modalidade: str
    informacoes_adicionais: str
    edital_disponivel: str
    data_inicio: str
    valor_mensalidade: str

# Ferramenta para raspagem
scraper_tool = ScrapeWebsiteTool()

# Agente de extra√ß√£o
agente_extracao = Agent(
    role="Agente de Coleta de Informa√ß√µes de Cursos",
    goal="Extrair informa√ß√µes detalhadas do curso de p√≥s-gradua√ß√£o da UFG",
    backstory="Especialista em web scraping focado em dados educacionais e informa√ß√µes acad√™micas.",
    verbose=True,
    memory=True,
    llm=llm,
    #max_rpm=1,
    tools=[scraper_tool]
)

# Tarefa de extra√ß√£o
extrair_informacoes_site = Task(
    description=f"""
    Acesse o site {url} e extraia TODAS as informa√ß√µes do curso de p√≥s-gradua√ß√£o.
    
    Voc√™ DEVE coletar exatamente estes campos:
    - nome: Nome completo do curso
    - descricao: Descri√ß√£o resumida do curso
    - carga_horaria: Carga hor√°ria total (ex: "360 horas")
    - modalidade: Se √© online, presencial ou h√≠brido
    - informacoes_adicionais: Informa√ß√µes relevantes sobre o curso
    - edital_disponivel: "Sim" ou "N√£o" se h√° edital dispon√≠vel
    - data_inicio: Data de in√≠cio das aulas
    - valor_mensalidade: Valor da mensalidade (ex: "R$ 500,00" ou "Gratuito")
    
    IMPORTANTE: Se alguma informa√ß√£o n√£o estiver dispon√≠vel no site, use "N√£o informado".
    """,
    expected_output="Informa√ß√µes estruturadas do curso em formato de dicion√°rio Python",
    tools=[scraper_tool],
    agent=agente_extracao
)

# Agente formatador
agente_formatador_json = Agent(
    role="Especialista em Formata√ß√£o de Dados",
    goal="Converter as informa√ß√µes extra√≠das em formato JSON v√°lido e bem estruturado",
    backstory="Especialista em estrutura√ß√£o de dados com foco em precis√£o e padroniza√ß√£o.",
    verbose=True,
    memory=True,
    #max_rpm=1,
    llm=llm
)

# Tarefa de formata√ß√£o
formatar_json = Task(
    description="""
    Receba as informa√ß√µes extra√≠das e formate em JSON v√°lido.
    
    O JSON deve ter EXATAMENTE esta estrutura:
    {
        "curso": {
            "nome": "...",
            "descricao": "...",
            "carga_horaria": "...",
            "modalidade": "...",
            "informacoes_adicionais": "...",
            "edital_disponivel": "...",
            "data_inicio": "...",
            "valor_mensalidade": "..."
        }
    }
    
    Retorne APENAS o JSON, sem texto adicional antes ou depois.
    Use "N√£o informado" para campos sem informa√ß√£o.
    """,
    expected_output="JSON v√°lido contendo todas as informa√ß√µes do curso",
    agent=agente_formatador_json,
    output_file="resultado_scraping.json"  # Salvamento autom√°tico
)

# Criar equipe
equipe = Crew(
    agents=[agente_extracao, agente_formatador_json],
    tasks=[extrair_informacoes_site, formatar_json],
    process=Process.sequential,
    verbose=True
)

# Executar
print("üöÄ Iniciando extra√ß√£o de dados...")
resultado = equipe.kickoff(inputs={'url': url})

print("\n" + "="*60)
print("üìä RESULTADO DA EXTRA√á√ÉO")
print("="*60)
print(resultado)

# Salvar resultado em JSON
try:
    # Tentar parsear se o resultado j√° for uma string JSON
    if isinstance(resultado, str):
        # Remover poss√≠veis marcadores de c√≥digo markdown
        resultado_limpo = resultado.strip()
        if resultado_limpo.startswith("```"):
            linhas = resultado_limpo.split("\n")
            resultado_limpo = "\n".join(linhas[1:-1])
        
        saida_json = json.loads(resultado_limpo)
    else:
        # Se for objeto, converter para dict
        saida_json = resultado if isinstance(resultado, dict) else {"resultado": str(resultado)}
    
    # Salvar arquivo
    with open("resultado_scraping.json", "w", encoding="utf-8") as f:
        json.dump(saida_json, f, ensure_ascii=False, indent=2)
    
    print("\n‚úÖ Resultado salvo em 'resultado_scraping.json'")
    
    # Mostrar pr√©via do JSON salvo
    print("\nüìÑ Pr√©via do JSON salvo:")
    print(json.dumps(saida_json, ensure_ascii=False, indent=2))

except json.JSONDecodeError as e:
    print(f"\n‚ö†Ô∏è Erro ao parsear JSON: {e}")
    print("Salvando resultado como texto bruto...")
    
    # Salvar como objeto com o resultado bruto
    fallback_json = {
        "status": "warning",
        "mensagem": "Resultado n√£o estava em formato JSON v√°lido",
        "resultado_bruto": str(resultado)
    }
    
    with open("resultado_scraping.json", "w", encoding="utf-8") as f:
        json.dump(fallback_json, f, ensure_ascii=False, indent=2)
    
    print("‚úÖ Resultado salvo em 'resultado_scraping.json' (formato alternativo)")

except Exception as e:
    print(f"\n‚ùå Erro ao salvar arquivo: {e}")
    print("Resultado bruto:")
    print(resultado)