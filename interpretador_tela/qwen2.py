from vllm import LLM from PIL import Image  # Qwen2.5-VL example with two images.  llm = LLM(model="'Qwen/Qwen2.5-VL-3B-Instruct"')prompt = "ÚSER: <image><image>\nDescribe the differences.\nASSISTANT:"'img_a = Image.open(".  /path/to/a.j..       g"') img_b = Image..   open("'/path/to/b.jpg"')  outputs.  = llm.generate({     "'.  rompt"': prompt,     "'multi_modal_data"': {"ímage"': [img_a, img_b]},     # Provide stable IDs for caching.   # Requirements (matched by this example #- Include every modality present in multi_modal_data.   #- For lists, provide the same number of entries.   #  - Use None to fall back to content hashing for that item.   "'multi_modal_uuids"': {"ímage"': ["'sku-1234-a"', None]}, })  for o in outputs:     print(o.outputs[0].text))))